# ==============================================================================
# context: teaching - Shared Educational Resources in Computer Science
#          ENIB: module IAS - Lab work on neural networks (since fall'21)
#                           - student project (fall'21)
# description: runs the learning algorithm to fit the model to the data 
# copyright (c) 2018-2021 ENIB. All rights reserved.
# ------------------------------------------------------------------------------
# usage: python learnModel <datadir> <filenamePrefix>
#   <datadir>: the directory where to find the data (and where the model will be saved to)
#   <filenamePrefix>: the prefix of the files containing the various data subsets, 
#   csv files generated by splitDataSet
# dependencies:
# - python 3 (see import statements)
# - structure of the data set files
# - range of the variables
# tested with:
# - python 3.8.5 on macOS 11.3 (from shell)
# - python 3.9.6 on windows 10 (from anaconda prompt within conda env)
# ------------------------------------------------------------------------------
# creation: 17-sep-2018 pierre.chevaillier@enib.fr
# revision: 26-aug-2021 pierre.chevaillier@enib.fr code cleaning, new org.
# revision: 09-sep-2021 pierre.chevaillier@enib.fr code cleaning, print messages
# revision: 15-sep-2021 pierre.chevaillier@enib.fr code cleaning, dummy values / some hyper-parameters
# ------------------------------------------------------------------------------
# comments:
# -
# warnings:
# - for educational purposes only
# todos:
#  - see TODO
#  - Critical: adapt to the learning data
#  - Improvemnt: more controls on the command line arguments
# ==============================================================================

# Python standard distribution
import sys
import os
import datetime

# PORT: decimal separator for savetxt / loadtxt <>
import locale
locale.getlocale()

# Specific modules
import numpy as np

# Machine Learning library
from keras.models import Sequential
from keras.layers import Dense

# Home made stuffs
# None

# ==============================================================================
# Global variables:
eol = '\n' # SEE: https://docs.python.org/3/library/os.html (see os.linesep)
csv_sep = ';'

xDim = 2 # input variables (distance to and azimuth of the agent's target)
yDim = 2 # output variables (linear and angular velocities of the agent)

# Hyper parameters
# TODO: set the two following values (try different settings)
hiddenLayers = [1]
hiddenActivationFunctionName = 'tanh'
outputActivationFunctionName = 'linear'
optimizerName = 'adam'
lossFunctionName = 'mse'
nMaxEpochs = 2

# ==============================================================================
# Locally defined functions

def learnModel(pathToLearningDataDir, fileNamePrefix):

    # --- Load the data sets (training and validation)
    pathToDataFile = os.path.join(pathToLearningDataDir, fileNamePrefix + '_training.csv')
    print("Load training data set from", pathToDataFile)
    data = np.loadtxt(pathToDataFile, dtype = float, delimiter = csv_sep)
    X_train, Y_train = data[:,[0,1]], data[:,[2,3]]

    pathToDataFile = os.path.join(pathToLearningDataDir, fileNamePrefix + '_validation.csv')
    print("Load validation data set from", pathToDataFile)
    data = np.loadtxt(pathToDataFile, dtype = float, delimiter = csv_sep)
    X_valid, Y_valid = data[:,[0,1]], data[:,[2,3]]

    # --- Build the model
    neuralNetwork = Sequential() # layered neural architecture

    # Stack the layers
    if len(hiddenLayers) == 0:
        print("Error: you did not set the number of neurones per hidden layers")
        return

    # Input layer
    # In keras the input layer is created within the creation of the first hidden layer
    layer = Dense(hiddenLayers[0],
        input_dim = xDim, 
        activation = hiddenActivationFunctionName)
    neuralNetwork.add(layer)
  
    # Stack other hidden layers (if any)
    layerSizes = hiddenLayers[1:]
    for layerSize in layerSizes:
        if layerSize > 0:
            layer = Dense(layerSize, activation = hiddenActivationFunctionName)
            neuralNetwork.add(layer)
  
    # output layer
    layer = Dense(yDim, activation = outputActivationFunctionName)
    neuralNetwork.add(layer)
  
    # --- Set the learning algorithm
    neuralNetwork.compile(optimizer = optimizerName, loss = lossFunctionName)

    # --- Fit the model (learning, training per se)
    history = neuralNetwork.fit(X_train, Y_train,
            epochs = nMaxEpochs,
            batch_size = 32,  #default: 32
            validation_data = (X_valid, Y_valid),
            verbose = 1)
  
    fileNamePrefix = defineModelFileNamePrefix() # for files' naming

    # --- Evaluate the learning process
    nEpochs = len(history.history['loss'])
    print("Values of the loss (" + lossFunctionName + ") after epoch #"+ str(nEpochs))
    if nEpochs > 0:
        trainLosses = history.history['loss']
        validLosses = history.history['val_loss']
        print("\tOn the training data subset:\t" +  str(trainLosses[nEpochs-1]))
        print("\tOn the validation data subset:\t" + str(validLosses[nEpochs-1]))
        saveLearningCurve(trainLosses, validLosses,fileNamePrefix, pathToLearningDataDir)

    # -- Save the trained model for further usage (predictions)
    trainedModelFileName = fileNamePrefix + '_model.h5'
    pathToTrainedModelFile = os.path.join(pathToLearningDataDir, trainedModelFileName)
    print("Save trained model to", pathToTrainedModelFile)
    neuralNetwork.save(pathToTrainedModelFile)

    return

# ------------------------------------------------------------------------------
def saveLearningCurve(trainLosses, validLosses, fileNamePrefix, pathtoDir):
    fileName = fileNamePrefix + '_loss.csv'
    pathToFile = os.path.join(pathtoDir, fileName)
    print("Save loss values to", pathToFile)
    file = open(pathToFile, 'w+')
    for epoch in range(0,len(trainLosses)):
        file.write(str(trainLosses[epoch]) + csv_sep + str(validLosses[epoch]) + eol)
    file.close()
    return

# ------------------------------------------------------------------------------
def defineModelFileNamePrefix():
    fileNamePrefix = 'mlp_' + hiddenActivationFunctionName + '_' + outputActivationFunctionName + '_'
    nLayers = len(hiddenLayers)
    for i in range(0, nLayers):
        fileNamePrefix += str(hiddenLayers[i])
        if i < nLayers - 1:
            fileNamePrefix += 'x'
    fileNamePrefix += '_' + datetime.datetime.now().strftime("%Y-%m-%d-%H-%M-%S")
    return fileNamePrefix

# ==============================================================================
if __name__ == "__main__":

    # Where to read and write the different files
    if len(sys.argv) > 1:
        pathToLearningDataDir = sys.argv[1]
    else:
        pathToLearningDataDir = '.'
    if not os.path.isdir(pathToLearningDataDir):
        print("Error: learning data directory " + pathToLearningDataDir + " does not exist.")
        sys.exit(1)
    
    if len(sys.argv) > 2:
        fileNamesPrefix = sys.argv[2]
    else:
        print("Error: provide the prefix of the learning data files' name")
        sys.exit(2)

    learnModel(pathToLearningDataDir, fileNamesPrefix)

# end of file
# ==============================================================================
